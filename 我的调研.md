# 一、工业界VLA模型在商用机器人上的使用

## 1.目前现状

目前来说，对于家庭中非结构化、强语义依赖的场景，VLA凭借其强大的语义理解和零样本泛化能力更具优势。但是对于工厂结构化的场景，模仿学习（IL）和强化学习（RL）仍是落地首选。

在模仿学习的框架下，采集数千条人类示教数据，可以教会机器人干一些协作机械臂的活。也可以选择强化学习，即通过先建立仿真环境，通过RL大量交互，利用Sim-to-Real技术训练一个basemodel，然后再在真实环境中通过人类示教数据做微调。

但是这种“专用模型”的边际成本极高。面对现实中多变产线和碎片化任务，每个任务都需要在现实场景下采集数千条的演示数据，这样做的金钱和时间成本都会非常高昂，而且难以规模化复制。因此，工业界正逐步探索将VLA的泛化能力引入长尾任务，试图解决单一模型应对多任务的泛化难题。

![image-20260201183112663](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201183112663.png)

![image-20260201183053591](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201183053591.png)





![image-20260201182931036](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201182931036.png)

 

## 2.市面上其他公司的VLA使用情况

 

| 公司名称   | VLA方案       | 网站                                                         | 特点                                                         |
| ---------- | ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Nvidia     | GR00T  N1     | [paper](https://arxiv.org/pdf/2503.14734) [精读](https://blog.csdn.net/v_JULY_v/article/details/146376514?fromshare=blogdetail&sharetype=blogdetail&sharerId=146376514&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 采用冻结的 Eagle VLM 搭配简化的适配器架构，并引入FLARE（未来潜变量对齐）损失函数。通过结合DreamGen 生成的合成轨迹与多源真实/仿真数据。 |
|            | GR00T  N1.5   | [website](https://research.nvidia.com/labs/gear/gr00t-n1_5/) [github](https://github.com/NVIDIA/Isaac-GR00T/tree/main) [精读](https://blog.csdn.net/v_JULY_v/article/details/151907116?fromshare=blogdetail&sharetype=blogdetail&sharerId=151907116&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) |                                                              |
| Figure     | Helix 01      | [website](https://www.figure.ai/news/helix) [精读](https://blog.csdn.net/v_JULY_v/article/details/145773235?fromshare=blogdetail&sharetype=blogdetail&sharerId=145773235&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 快慢双系统。视频来监督微调VLM。端到端训练，优化的流式推理    |
|            | Helix 02      | [website](https://www.figure.ai/news/helix-02)               |                                                              |
| Tesla      | FSD           |                                                              | 端到端                                                       |
| 银河通用G1 | Grasp VLA     | [website ](https://pku-epic.github.io/GraspVLA-web/)[github](https://github.com/PKU-EPIC/GraspVLA) [paper](https://arxiv.org/pdf/2505.03233) [精读](https://blog.csdn.net/v_JULY_v/article/details/156657685?fromshare=blogdetail&sharetype=blogdetail&sharerId=156657685&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 基于十亿级合成动作数据集SynGrasp-1B预训练的抓取基础模型。该模型采用自回归感知与流匹配动作生成的统一架构，展现了在不同光照、背景及干扰下的强大零样本泛化能力。 |
|            | Track VLA     | [website](https://pku-epic.github.io/TrackVLA-web/)          | 野外具身视觉跟踪。主要用于四足                               |
|            | Grocery VLA   |                                                              |                                                              |
|            | Stereo VLA    | [website](https://shengliangd.github.io/StereoVLA-Webpage/)  | 立体视图用于空间感知，单目视图用于指令追踪。对相机姿态变化有强鲁棒性 |
| 智元       | GO-1          | [website](https://agibot-world.com/blog/go1) [github](https://github.com/OpenDriveLab/AgiBot-World) [paper](https://agibot-world.com/blog/agibot_go1.pdf) [精读](https://blog.csdn.net/v_JULY_v/article/details/146176858?fromshare=blogdetail&sharetype=blogdetail&sharerId=146176858&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 开源？视觉-语言-潜在动作（ViLLA）框架。融合了VLM和MoE（潜在规划器和动作专家）隐式规划器：机器人“内心戏” |
| 星海图     | G0/G0Plus     | [website](https://opengalaxea.github.io/GalaxeaVLA/) [github](https://github.com/OpenGalaxea/GalaxeaVLA) [paper](https://arxiv.org/pdf/2509.00576) | 开源？多模态规划的视觉语言模型（VLM）与用于细粒度执行的视觉语言动作（VLA）模型相结合 |
| 自变量     | WALL-OSS      | [website](https://x2robot.com/en/research/68bc2cde8497d7f238dde690) [github](https://github.com/X-Square-Robot/wall-x) [paper](https://arxiv.org/pdf/2509.11766) | 开源？采用紧密耦合的架构和多策略训练课程，实现了统一的跨层CoT（思维链）：在一个可微分的框架内无缝地统一了指令推理、子目标分解和细粒度动作合成。 |
| 智平方     | GOVLA         | [website ](https://fast-in-slow.github.io/)[github](https://github.com/CHEN-H01/Fast-in-Slow) [paper](https://arxiv.org/pdf/2506.01953) | 开源？将快慢系统统一，通过部分参数共享，将系统1的执行模块嵌入到基于VLM的系统2中。两者以异构模态输入并以异步频率运行。 |
| 宇树科技   | UnifoLM-VLA-0 | [website](https://unigen-x.github.io/unifolm-vla.github.io/) [github](https://github.com/unitreerobotics/unifolm-vla) |                                                              |
| 星动纪元   |               |                                                              |                                                              |

 



### Figure

Helix，其核心在于采用了类比人类认知的“快慢双系统”设计：由7B参数的VLM（System2，慢系统）以7-9Hz低频负责全局语义理解和规划，生成潜在向量（Latent Vector）。再由80M参数的视觉策略网络（System1，快系统）结合该向量与实时观测，以200Hz高频输出精准的动作控制。这种架构使得机器人无需针对特定任务微调（Zero-shot/Few-shot）即可完成如叠衣服、物流分拣及双机协作等复杂任务，实现了通用性与实时性的统一。



- **Figure 01**

  整个流程为：**图像 + speech to text 、VLM接收并做综合处理 、NNP输出执行策略、WBC执行策略且将VLM处理得到的response speak出来**，具体则如下

<img src="./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1cc304571bc7fada876e5b564e423e53.png" alt="img" style="zoom:50%;" />

 

### **Helix 01**

<img src="./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131230918043.png" alt="image-20260131230918043" style="zoom:50%;" />

- **Scaling Law**

<img src="./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131230942955.png" alt="image-20260131230942955" style="zoom:50%;" />

- **快慢双系统**

![image-20260131231043266](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231043266.png)

|          | **系统2(S2)-“慢思考”**                       | **系统1(S1)-“快反应”**                                       |
| -------- | -------------------------------------------- | ------------------------------------------------------------ |
| 定位     | 高级语义规划器(Planner)                      | 低级动作执行器(Actor)                                        |
| 模型     | 开源VLM(在互联网数据预训练)                  | 视觉编码器(交叉注意力Transformer)                            |
| 参数     | 7B                                           | 80M                                                          |
| 频率     | 7-9Hz                                        | ~200Hz                                                       |
| 功能     | 场景理解、语义解析、跨场景/对象泛化          | 快速推理、闭环控制、动作生成                                 |
| 输入     | 1.单目图像+机器人状态(腕/指) 2.自然语言指令  | 1.单目图像+机器人状态(同S2) 2.来自S2的潜在向量(Latent Vector) |
| 处理逻辑 | 将视觉和语言信息投影到嵌入空间，提炼语义信息 | 1.视觉骨干网处理图像(全卷积/多尺度) 2.将S2向量投影并与视觉特征拼接(Concat) |
| 训练     | 互联网规模数据预训练                         | 完全在模拟环境中预训练初始化                                 |
| 输出     | 单一连续潜在向量(Latent Vector)              | 具体的机器人动作策略                                         |

![image-20260131231059140](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231059140.png)

![image-20260131230846267](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131230846267.png)

- **模型和训练**

![image-20260131231555290](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231555290.png)

![image-20260131231606707](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231606707.png)

![image-20260131231632993](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231632993.png)

![image-20260131231648838](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260131231648838.png)



### Helix 02

**Helix 02 是 Figure 迄今为止功能最强大的人形机器人：**它采用单一神经系统，可直接从像素控制全身，从而在整个房间内实现灵巧、远距离的自主移动。Helix 02 代表了多项突破：

- **自主长距离移动操作：** Helix 02 能够在一个完整的厨房内完成洗碗机的装卸工作——这项全程自主任务耗时四分钟，整合了行走、操作和平衡能力，无需任何重置或人工干预。我们相信这是迄今为止人形机器人自主完成的最长距离、最复杂的任务。
- **所有传感器输入，所有执行器输出：** Helix 02 通过一个统一的视觉运动神经网络，将每个板载传感器（视觉、触觉和本体感觉）直接连接到每个执行器**。**
- **基于人体数据的类人全身控制**：所有结果均由**系统 0**实现，该系统是一个经过训练的全身控制器，基于超过 1000 小时的人体运动数据和仿真到实际的强化学习进行训练。系统 0 用一个神经网络先验取代了 109,504 行手工编写的 C++ 代码，从而实现了稳定、自然的运动。
- **新的灵巧度类别：**借助 Figure 03 的嵌入式触觉传感和掌心摄像头，Helix 02 可以执行以前无法完成的操作：取出单个药丸、分配精确的注射器容量，以及在自身遮挡的情况下从杂乱中取出细小的、不规则的物体。



![image-20260201194655903](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201194655903.png)

每个系统都以其固有的时间尺度运行。 S0 提供学习到的全身控制，S1 将所有传感器连接到所有执行器，S2 实现对扩展任务的语义推理，它们共同构成了一个从像素到扭矩的紧密整合的层级结构。

**系统2 (S2)**缓慢地进行目标推理：解读场景、理解语言和组织行为。

**系统1 (S1)**思维敏捷，以200赫兹的频率将感知转化为全身关节目标。

**系统0 (S0)**以1千赫兹的频率执行，负责处理全身的平衡、接触和协调。

- **系统 0：基于人体数据的类人全身控制**

S0 是类人全身控制的基础模型：它预先学习了人们如何在保持平衡和稳定的情况下运动。它是 Helix 02 物理具身化的基石：当更高层级负责推理任务和计划时，S0 则确保每个动作都能流畅、安全、稳定地执行。

S0 并非为行走、转身、蹲伏或伸手等动作分别设计奖励函数，而是直接从庞大且多样化的运动数据集中学习追踪人体运动。在学习复现这些运动的过程中，该策略学会了如何协调力、调整姿势，并在实现一般移动操控所需的所有行为中保持平衡。

**训练数据：**超过 1000 小时的关节级重定向人体运动数据。

**架构：**一个 1000 万参数的神经网络，以全身关节状态和基座运动作为输入，以 1 kHz 的频率输出关节级执行器命令。

**仿真训练：** S0 完全在仿真环境中进行训练，在超过 20 万个并行环境中进行广泛的领域随机化，从而能够直接迁移到真正的机器人并在整个机器人集群中进行泛化。

- **系统 1：“所有传感器都输入，所有关节都输出” 视觉运动策略**

在初代Helix中，S1控制上半身并读取关节状态和图像。在Helix 02中，它连接所有传感器，控制整个机器人。

- **输入设备**：头部摄像头、掌心摄像头、指尖触觉传感器和全身本体感觉传感器。
- **输出**：对整个机器人进行完全关节级控制——腿部、躯干、头部、手臂、手腕和单个手指。

这种**像素到全身的**架构使S1能够将机器人和环境作为一个单一耦合系统进行整体状态推理。掌部摄像头和触觉传感器是图片所示的新硬件功能。这是我们首次展示依赖于这些模态的神经网络策略。

当物体被遮挡而无法被头部摄像头捕捉到时，**掌心摄像头可提供手部视觉反馈。每个指尖都嵌入了触觉传感器**，能够探测到低至3克的力——灵敏度足以感知回形针——从而实现接触感知、力感应式抓握。这些传感方式使Helix能够充分发挥五指手的灵巧性，轻松应对需要多指抓握精细运动控制的复杂操作任务。

系统1 仍然是受系统 2 潜在信号影响的变压器，但现在产生全身关节目标，系统0 以 kHz 速率跟踪这些目标。

- **系统2：场景理解和语言**

S2仍然是语义推理层：处理场景、理解语言并为S1生成潜在目标。Helix 02极大地扩展了系统2 可以指定的行为*范围。之前：* *“拿起番茄酱。”*现在：

- “走到洗碗机旁，打开它。”
- “把碗拿到柜台上去。”
- “回到上层架子上，把杯子拿过来。”

S2 不需要规划低层次的脚步动作，也不需要具体说明如何协调手臂和腿部的运动。它生成一系列语义潜意识，S1 将其解读为运动指令，然后由 S0 执行。

- **结果：自主长距离移动操作**

Helix 02 执行持续数分钟的任务，需要完全整合运动、灵巧性和感知能力。这段持续 4 分钟的操作序列是迄今为止演示过的最复杂的自主操作序列。这也是首次在人形机器人上演示如此长时程、端到端的“像素到全身”控制。

**这说明了什么**：

- *在操作约束下的运动*：机器人能够一边行走一边抓取易碎物品，并在每一步都保持稳定的抓握。
- *充分利用全身*：当双手被占用时，机器人会用臀部关上抽屉，用脚抬起洗碗机门——将整个身体作为工具，而不是仅仅依靠双手。
- *双手协调配合贯穿始终*：物体被拿起、在双手之间转移、堆叠和放置，而双臂则作为一个协调的系统进行操作。
- *跨尺度的运动范围*：同一个神经网络既能产生毫米级的指尖运动，也能产生房间尺度的移动——动态范围跨越四个数量级。
- *长时域序列执行*：61 个运动操作动作，顺序正确，并具有隐式错误恢复能力。机器人可在数分钟的执行时间内保持任务状态。

- **结果：灵巧操作与触觉和手内视觉**

Helix 02 的触觉传感和掌部摄像头突破了纯视觉策略的限制，实现了更多操作任务。



### Tesla FSD

FSD推特博文**Tesla's approach to Autonomy**

https://x.com/aelluswamy/status/1981644831790379245?s=20

- **为什么要采用端到端的方式？**

尽管特斯拉坚信端到端神经网络技术，但这绝非自动驾驶领域的共识方案。大多数其他自动驾驶研发公司都采用传感器密集型、模块化的驾驶方式。虽然这类系统在初期开发和调试可能更容易，但其复杂性也不容忽视。相比之下，端到端方案具有诸多优势。例如：

1. 将人类价值观系统化极其困难，从数据中了解它们则容易得多。
2. 感知、预测和规划之间的接口定义不明确。**在端到端架构中，梯度从控制端一直流向传感器输入端，从而对整个网络进行整体优化**。
3. **易于扩展**，可处理现实世界机器人技术的庞大而长尾需求。
4. **具有确定性延迟的同构计算**。

- **挑战1：维度诅咒**

在实际环境中安全运行需要处理高帧率、高分辨率、长时间的上下文输入。如果我们对“输入标记”的大小做出一些合理的假设，比如一个 5x5 像素的区域，那么最终得到的标记数量如下。

1. 7 个摄像头 x 36 帧/秒 x 500 万像素 x 30 秒历史记录 / (5x5 像素块)
2. 接下来几英里的导航地图和路线
3. 100 Hz 运动学数据，例如速度、IMU、里程计等
4. 48 kHz 音频数据 

这相当于大约20亿个输入标记。神经网络需要学习正确的因果映射，将这20亿个标记简化为两个标记：车辆的下一次转向和加速。在不学习虚假相关性的情况下学习正确的因果关系是一个极其棘手的问题。

幸运的是，特斯拉拥有海量数据，其车队每天产生的行驶数据量相当于500年的行驶里程。并非所有数据都具有价值，而且收集所有数据也不切实际。因此，特斯拉利用复杂的数据引擎管道来筛选出最有趣、最多样化且最高质量的数据样本。

- **挑战2：可解释性与安全性**

如果车辆运行不符合预期，调试这样一个端到端系统可能会很困难。但实际上，这并不是什么大问题，因为模型还可以**生成可解释的中间标记。**根据具体情况，这些中间标记也可以用作推理标记。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871584529-23.png)

特斯拉的生成式高斯散射算法就是这样一个例子。虽然近年来**3D高斯散射算法**在计算机视觉领域取得了长足进步，但它依赖于具有较大基线的摄像头视角才能获得良好的性能。然而，典型的车辆运动通常是线性的，因此运行传统的高斯散射算法会导致重建质量较差，尤其是在从新视角进行重建时。此外，这些3D高斯散射算法还需要其他流程进行良好的初始化，并且总优化时间可能长达数十分钟。

另一方面，特斯拉的生成式高斯散射算法具有极佳的泛化能力，运行时间仅约 220 毫秒，无需初始化，能够对动态物体进行建模，并且可以与端到端 AI 模型联合训练。值得注意的是，所有这些高斯分布都是基于量产车辆配置的摄像头数据生成的。

除了 3D 几何图形外，该推理模型还可以结合自然语言和视频进行推理。FSD v14.x 版本中已经运行了该推理模型的简化版本。

- **挑战3：评估**

最后也是最棘手的挑战是评估。即使拥有高质量的数据集，开环预测的损失也未必能反映出实际应用中的优异性能。评估需要多样化且覆盖多种模式，才能实现快速的开发迭代。这项工作十分繁琐，需要投入大量精力才能在评估指标中获得高信噪比。

正因如此，特斯拉开发了神经世界模拟器。该模拟器使用我们精心收集的海量数据集进行训练。然而，与根据状态预测动作不同，神经世界模拟器能够根据当前状态和下一步动作合成未来状态。然后，它可以与智能体或策略AI模型连接起来，以闭环方式运行，从而评估性能。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871587034-26.png)

该世界模拟器完全由特斯拉训练，用于生成车辆所有摄像头和其他传感器的数据。它具有因果关系，并能响应驾驶策略模型的指令。它速度快，同时还能合成高分辨率、高帧率和高质量的传感器数据。

这种模拟可用于根据历史数据验证新的驾驶模型。此外，我们还可以合成新的对抗场景，以测试其他极端情况。通过调整测试时计算量，同一模型可以实时模拟现实世界。以上所有要点最棒的地方在于，它们不仅能解决车辆自动驾驶问题，还能无缝地应用到特斯拉人形机器人Optimus上。

显然，上述所有视频生成过程并不局限于评估。它还可以用于闭环大规模强化学习，从而实现超越人类的性能。

###  Tesla Optimus





### 银河通用

- **Grasp VLA**

Grasp VLA，一个在大规模合成动作数据上预训练的 VLA 模型，作为抓取任务的基础模型。Grasp VLA将自回归感知任务和基于流量匹配的动作生成整合为统一的思维链过程，支持对合成动作数据和互联网语义数据的联合训练。这种设计有助于减少模拟到现实的差距，并促进将学习到的动作转移到更广泛的互联网覆盖对象上，实现了掌握中的开放词汇泛化。

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871590333-29.png)

- **SynGrasp-1B**

SynGrasp-1B，这是一个包含十亿帧的模拟抓取数据集，具有逼真的渲染效果和广泛的领域随机化，包括初始机器人姿态、物体姿态、背景、光照和材质。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871592784-32.png)

- **模型**

GraspVLA由自回归视觉语言骨干网络和基于流匹配的动作专家组成。它利用互联网基础数据和合成动作数据之间的协同作用，采用渐进式动作生成机制：该模型首先预测合成数据和网络数据中目标对象的二维边界框，然后针对合成数据生成抓取姿态和分块动作。

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871596270-35.png)

 

### 智元GO-1

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/framework.png)

为了有效利用高质量的AgiBot World数据集以及互联网大规模异构视频数据，增强策略的泛化能力，智元提出了 Vision-Language-Latent-Action (ViLLA) 这一创新性架构。GO-1作为首个通用具身基座大模型，基于ViLLA构建。与 Vision-Language-Action (VLA) 架构相比，ViLLA 通过预测**Latent Action Tokens(隐式动作标记)**，弥合图像-文本输入与机器人执行动作之间的鸿沟。在真实世界的灵巧操作和长时任务方面表现卓越，远远超过了已有的开源SOTA模型。

ViLLA架构是由**VLM(多模态大模型) + MoE(混合专家)**组成，其中VLM借助海量互联网图文数据获得通用场景感知和语言理解能力，MoE中的Latent Planner(隐式规划器)借助大量跨本体和人类操作数据获得通用的动作理解能力，MoE中的Action Expert(动作专家)借助百万真机数据获得精细的动作执行能力。在推理时，VLM、Latent Planner和Action Expert三者协同工作：

1. VLM 采用InternVL-2B，接收多视角视觉图片、力觉信号、语言输入等多模态信息，进行通用的场景感知和指令理解
2. Latent Planner是MoE中的一组专家，基于VLM的中间层输出预测Latent Action Tokens作为CoP(Chain of Planning，规划链)，进行通用的动作理解和规划
3. Action Expert是MoE中的另外一组专家，基于VLM的中间层输出以及Latent Action Tokens，生成最终的精细动作序列

下面展开介绍下MoE里2个关键的组成Latent Planner和Action Expert：

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/moe.png)



**混合专家一：Latent Planner（隐式规划器）**

尽管AgiBot World 数据集已经是全球最大的机器人真机示教数据集，但这样高质量带动作标签的真机数据量仍然有限，远少于互联网规模的数据集。为此，我们采用Latent Actions（隐式动作）来建模当前帧和历史帧之间的隐式变化，然后通过Latent Planner预测这些Latent Actions，从而将异构数据源中真实世界的动作知识转移到通用操作任务中。

- Latent Action Model（LAM，隐式动作模型）主要用于获取当前帧和历史帧之间Latent Actions的Groundtruth（真值），它由编码器和解码器组成。其中
  - 编码器采用Spatial-temporal Transformer，并使用Causal Temporal Masks（时序因果掩码）。
  - 解码器采用Spatial Transformer，以初始帧和离散化的Latent Action Tokens作为输入。
  - Latent Action Tokens通过VQ-VAE的方式进行量化处理。
- Latent Planner负责预测这些离散的Latent Action Tokens，它与 VLM 主干网络共享相同的 Transformer 结构，但使用了两套独立的FFN(前馈神经网络)和Q/K/V/O(查询、键、值、输出)投影矩阵。Latent Planner这组专家会逐层结合 VLM 输出的中间信息，通过Cross Entropy Loss（交叉熵损失）进行监督训练



**混合专家二：Action Expert（动作专家）**

为了实现 High-frequency（高频率）且 Dexterous（灵活）的操控，我们引入Action Expert，其采用Diffusion Model作为目标函数来建模低层级动作的连续分布。

- Action Expert结构设计上与Latent Planner类似，也是与 VLM 主干网络共享相同的 Transformer 结构，但使用两套独立的FFN和Q/K/V/O投影矩阵，它通过Denoising Process（去噪过程）逐步回归动作序列。
- Action Expert与VLM、Latent Planner分层结合，确保信息流的一致性与协同优化。

 

### 星海图G0

G0模型采用VLM规划器（任务规划）与VLA执行器（动作执行）相结合的双系统架构，并基于GalaxeaR1Lite移动双臂机器人平台，实施了“跨本体预训练->单本体预训练->特定任务后训练”的三阶段训练策略。核心结论指出，基于Galaxea数据集的第二阶段（单本体预训练）对于提升模型在长程移动操作、小样本迁移及全身控制上的性能至关重要，显著优于仅依赖跨本体数据的方案。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871601222-41.png)

 

### 自变量WALL-OSS

WALL-OSS，这是一个端到端的具身基础模型，它利用大规模多模态预训练来实现（1）具身感知的视觉-语言理解，（2）强大的语言-动作关联，以及（3）稳健的操作能力。我们的方法采用紧密耦合的架构和多策略训练课程，实现了统一的跨层CoT——在一个可微分的框架内无缝地统一了指令推理、子目标分解和细粒度动作合成。

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871603840-44.png)

 

### 智平方GOVLA

Fast-in-Slow(FiS)，这是一种统一的双系统视觉-语言-动作（VLA）模型，它通过部分参数共享（partiallysharingparameters），将系统1执行模块嵌入到基于VLM的系统2中。这种创新范式不仅使系统1能够进行高频执行，而且还促进了系统2这一单个基础模型内部推理和执行组件之间的协调。

鉴于这两个系统在FiS-VLA中扮演着根本不同的角色，我们将它们设计为接受异构模态输入并以异步频率运行，从而实现既快速又精准的操作。为了实现两个系统之间的协调，我们提出了一种双感知协同训练策略（dual-awareco-trainingstrategy），在赋予系统1动作生成能力的同时，保留系统2的上下文推理表征。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871613857-47.png)

(a)与之前的双系统VLA方法（将一个独立的策略头作为系统1）不同，(b)FiS-VLA将完整VLM的最后几个Transformer块重新用作（repurposes）系统1，同时保留完整模型用于系统2的推理。在这种范式下，FiS-VLA实现了更优越的性能和高频控制，如(c)和(d)所示。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871615813-50.png)

FiS-VLA利用一个完整的VLM进行系统2推理，同时重新利用LLM的最后几个Transformer块作为系统1的执行模块。

- 系统2处理低频输入（如2D图像和语言指令），并产生中间潜在特征（latentfeatures），作为系统1的条件信息。
- 系统1不仅仅依赖于这些周期性更新的高层表征，它还处理包括3D点云、2D图像和机器人状态在内的高频输入，以产生稳定且响应迅速的动作。

为了进行联合优化，我们引入了一种双感知协同训练策略，该策略结合了扩散去噪目标（diffusion denoising objective）与自回归目标（auto regressive objective）。这使得FiS-VLA能够在支持快速动作生成的同时，保留系统2的多模态推理能力。

 



### 宇树科技

![image-20260201012636092](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201012636092.png)

![image-20260201012721868](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201012721868.png)



### 星动纪元









# 二、Physical Intelligence公司

Open Pi Github: [github](https://github.com/Physical-Intelligence/openpi)

|                                |                                                              | Paper摘要精简翻译                                            |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Pi 0开源                       | [paper](https://arxiv.org/abs/2410.24164) [website](https://www.physicalintelligence.company/blog/pi0) [精读](https://blog.csdn.net/v_JULY_v/article/details/143472442?fromshare=blogdetail&sharetype=blogdetail&sharerId=143472442&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) [源码剖析](https://blog.csdn.net/v_JULY_v/article/details/146068251?fromshare=blogdetail&sharetype=blogdetail&sharerId=146068251&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) [微调方法论](https://blog.csdn.net/v_JULY_v/article/details/146125555?fromshare=blogdetail&sharetype=blogdetail&sharerId=146125555&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 机器人学习在释放灵活、通用且灵巧的机器人系统的全部潜力，以及解决人工智能领域一些最深层次的问题方面前景广阔 。然而，要将机器人学习提升到构建有效现实世界系统所需的通用水平，目前在数据、泛化能力和鲁棒性方面仍面临重大障碍 。在本文中，我们探讨了通用机器人策略（即机器人基础模型）如何应对这些挑战，以及如何设计能够执行复杂且高度灵巧任务的有效通用机器人策略 。我们提出了一种**基于预训练视觉-语言模型（VLM）构建的新型流匹配（Flow Matching）架构**，旨在继承互联网规模的语义知识 。随后，我们讨论了如何在包含单臂机器人、双臂机器人和移动操作机器人等多种灵巧机器人平台的大规模多样化数据集上训练该模型。我们评估了模型通过直接提示执行任务的能力、遵循人类及高级 VLM 策略发出的语言指令的能力，以及通过微调习得新技能的能力 。 |
|                                | [LeRobot Pi 0剖析](https://blog.csdn.net/v_JULY_v/article/details/148370072?fromshare=blogdetail&sharetype=blogdetail&sharerId=148370072&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) |                                                              |
| Pi0-FAST开源                   | [paper](https://arxiv.org/pdf/2501.09747) [website](https://www.pi.website/research/fast) [pi0-FAST精读及源码解析](https://blog.csdn.net/v_JULY_v/article/details/145475733?fromshare=blogdetail&sharetype=blogdetail&sharerId=145475733&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 自回归序列模型，例如基于 Transformer 的视觉-语言-动作（VLA）策略，在捕捉复杂且具有泛化性的机器人行为方面成效显著。然而，此类模型要求我们对连续动作信号选择一种**标记化（tokenization）**方式，这决定了模型预测的离散符号如何映射为连续的机器人动作。我们发现，当前基于简单的逐维度、逐时间步分箱（binning）方案的机器人动作标记化方法，在利用高频机器人数据学习灵巧技能时，表现通常不佳。为应对这一挑战，我们提出了一种基于**离散余弦变换（DCT）**的新型压缩式机器人动作标记化方案。我们的标记化方法——**频域动作序列标记化（Frequency-space Action Sequence Tokenization，简称 FAST）**，使我们能够针对标准离散化方法完全失效的高度灵巧及高频任务，训练自回归 VLA 模型。基于 FAST，我们发布了 FAST+，这是一种通用的机器人动作标记器，并在 100 万条真实机器人动作轨迹上进行了训练。它可以作为一个“黑盒”标记器，用于处理涵盖多样化动作空间和控制频率的广泛机器人动作序列。最后，我们展示了当与 Pi 0 VLA 结合使用时，我们的方法能够扩展至 10,000 小时的机器人数据训练规模，并在性能上匹配扩散（diffusion）VLA 模型，同时将训练时间缩短多达 5 倍 。 |
| Hi Robot(大脑加强版pi 0)不开源 | [paper ](https://arxiv.org/pdf/2502.19417)[website](https://www.pi.website/research/hirobot) [精读](https://blog.csdn.net/v_JULY_v/article/details/147090612?fromshare=blogdetail&sharetype=blogdetail&sharerId=147090612&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 能够在开放世界环境中执行多种不同任务的通用机器人，不仅必须能够推理出完成目标所需的步骤，还必须能够处理复杂的指令、提示，甚至是任务执行过程中的反馈。错综复杂的指令（例如，“能不能给我做一个素食三明治？”或者“我不喜欢那个”）不仅要求机器人具备在物理上执行各个独立步骤的能力，还要求其能够将复杂的命令和反馈置于物理世界的情境中进行理解。在这项工作中，我们描述了一种在**分层结构**中使用视觉-语言模型（VLM）的系统。该系统首先对复杂的提示和用户反馈进行推理，以推断出完成任务最恰当的下一步骤，随后通过低层级的动作来执行该步骤。与那些只能完成简单命令（如“拿起杯子”）的直接指令跟随方法相比，我们的系统能够对复杂的提示进行推理，并在任务执行过程中结合情境化的反馈（如“那不是垃圾”）。我们在三种机器人平台上对该系统进行了评估，包括单臂机器人、双臂机器人和双臂移动机器人，展示了其处理清理凌乱桌面、制作三明治以及杂货购物等任务的能力。 |
| Pi 0.5 不开源                  | [paper ](https://www.pi.website/download/pi05.pdf)[website](https://www.pi.website/blog/pi05) [精读](https://blog.csdn.net/v_JULY_v/article/details/147443184?fromshare=blogdetail&sharetype=blogdetail&sharerId=147443184&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 为了让机器人变得有用，它们必须在实验室之外的现实世界中执行具有实际相关性的任务。尽管视觉-语言-动作（VLA）模型在端到端机器人控制方面已经展示了令人印象深刻的结果，但此类模型在野外（真实环境）中能泛化多远仍然是一个悬而未决的问题。我们描述了 Pi 0.5，这是一个基于 Pi 0 的新模型，**它利用在异构任务上的协同训练来实现广泛的泛化能力。**Pi 0.5 使用来自多种机器人的数据、高层语义预测、网络数据以及其他来源的数据，以实现具有广泛泛化能力的现实世界机器人操作。我们的系统使用一种**协同训练和混合多模态示例**的组合，这些示例结合了图像观测、语言指令、物体检测、语义子任务预测和低层动作。我们的实验表明，这种类型的知识迁移对于有效的泛化是必不可少的，并且我们首次证明了端到端学习赋能的机器人系统可以在全新的家庭中执行长程且灵巧的操作技能，例如清洁厨房或卧室。 |
| Pi 0.5Ki 改进版部分开源        | [paper ](https://arxiv.org/pdf/2505.23705)[website ](https://www.pi.website/research/knowledge_insulation)[精读](https://blog.csdn.net/v_JULY_v/article/details/149227640?fromshare=blogdetail&sharetype=blogdetail&sharerId=149227640&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 视觉-语言-动作（VLA）模型通过将端到端学习与源自网络规模视觉-语言模型（VLM）训练的语义知识迁移相结合，为训练物理系统（如机器人）的控制策略提供了一种强大的方法 。然而，**实时控制的约束往往与 VLM 的设计相冲突**：最强大的 VLM 拥有数百亿甚至数千亿的参数，这对实时推理构成了障碍，且它们基于离散标记（tokens）运作，而非控制机器人所需的连续值输出 。**为了应对这一挑战，近期的 VLA 模型采用了专门的模块来实现高效的连续控制，例如动作专家（action experts）或连续输出头（continuous output heads），这通常需要在预训练的 VLM 骨干网络中添加未经训练的新参数 。虽然这些模块提升了实时性和控制能力，但它们究竟是保留还是削弱了预训练 VLM 中蕴含的语义知识，以及它们对 VLA 训练动态有何影响，仍然是一个悬而未决的问题** 。在本文中，我们在包含连续扩散（diffusion）或流匹配（flow matching）动作专家的 VLA 背景下研究了这一问题，结果表明，简单直接地（naively）引入此类专家会显著损害训练速度和知识迁移 。我们对各种设计选择及其对性能和知识迁移的影响进行了广泛的分析，并提出了一种在 VLA 训练期间对 VLM 骨干网络进行**“隔离”（insulating）**的技术，从而有效缓解了这一问题 。 |
| RTC实时动作分块                | [paper ](https://arxiv.org/pdf/2506.07339)[follow-up paper](https://arxiv.org/pdf/2512.05964) [website](https://www.pi.website/research/real_time_chunking) [精读](https://blog.csdn.net/v_JULY_v/article/details/149352338?fromshare=blogdetail&sharetype=blogdetail&sharerId=149352338&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 现代人工智能系统，特别是那些与物理世界交互的系统，越来越需要实时性能。然而，最先进的通用模型，包括最近的视觉-语言-动作模型（VLA），其高延迟构成了一个重大挑战。**虽然动作分块（action chunking）已在高频控制任务中实现了时间一致性，但它并未完全解决延迟问题，导致在分块边界处出现停顿或分布外（out-of-distribution）的抖动动作。**本文提出了一种新颖的推理时算法，能够**实现动作分块策略的平滑异步执行**。我们的方法，即**实时分块（Real-Time Chunking，简称 RTC）**，适用于任何基于扩散（diffusion）或流（flow）的 VLA 模型，开箱即用，无需重新训练。**它在执行当前动作块的同时生成下一个动作块，“冻结”那些保证会被执行的动作，并“修复（inpainting）”剩余部分。**为了测试 RTC，我们在 Kinetix 模拟器中引入了一个包含 12 项高动态任务的新基准，并评估了 6 项具有挑战性的现实世界双手操作任务。结果表明，RTC 速度快、性能好，并且对推理延迟具有独特的鲁棒性，显著提高了任务吞吐量，并在诸如划火柴等精确任务中实现了高成功率——即使在存在显著延迟的情况下也是如此。 |
| Pi* 0.6不开源                  | [paper ](https://www.pi.website/download/pistar06.pdf)[website](https://www.pi.website/blog/pistar06) [精读](https://blog.csdn.net/v_JULY_v/article/details/154989166?fromshare=blogdetail&sharetype=blogdetail&sharerId=154989166&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 我们研究了视觉-语言-动作（VLA）模型如何通过强化学习（RL）在现实世界的部署中得到改进。我们提出了一种通用方法，即**“通过优势条件策略进行带有经验和修正的强化学习”（RECAP）**，该方法通过优势调节（advantage conditioning）实现了 VLA 的 RL 训练。我们的方法将异构数据整合到自我改进过程中，包括演示数据、来自同策略（on-policy）采集的数据，以及在自主执行期间提供的专家遥操作干预数据。RECAP 首先使用离线 RL（Offline RL）预训练一个通用的 VLA 模型，我们称之为  Pi * 0.6，随后可以通过在机器人上收集数据，使其针对下游任务进行专业化以获得高性能。我们展示了使用完整的 RECAP 方法训练出的 Pi * 0.6 模型能够在真实的家庭环境中折叠衣物、可靠地组装盒子，并使用专业意式咖啡机制作浓缩咖啡饮品。在一些最困难的任务上，RECAP 使任务吞吐量翻了一倍以上，并大约将任务失败率减半。 |
| Human to Robot                 | [paper ](https://www.pi.website/download/human_to_robot.pdf)[website](https://www.pi.website/research/human_to_robot) [精读](https://blog.csdn.net/v_JULY_v/article/details/156545611?fromshare=blogdetail&sharetype=blogdetail&sharerId=156545611&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 视觉-语言-动作（VLA）模型能够实现广泛的开放世界泛化，但需要大量且多样化的数据集 。考虑到人类视频涵盖了多样的现实世界场景且易于获取，探究是否可以从中使用部分数据是很有吸引力的 。然而，仅使用人类视频训练 VLA 模型非常困难，而且建立人类与机器人之间的映射需要手动工程，是一个重大的研究挑战 。受到大型语言模型进展的启发——即从多样化监督中学习的能力会随着规模的扩大而涌现——我们探讨这种现象是否也存在于包含人类视频数据的 VLA 模型中 。我们引入了一种简单的协同训练方法，并发现一旦 VLA 模型在足够多的场景、任务和具身（embodiments）上进行了预训练，人类到机器人的迁移能力就会涌现 。我们的分析表明，这种涌现的能力源于多样化的预训练为人类和机器人数据生成了与具身无关的表征 。我们通过一系列探究人类到机器人技能迁移的实验验证了这些发现，结果表明，在进行足够多样化的机器人预训练后，我们的方法可以将仅在人类数据中出现的泛化设置上的性能提高近一倍 。 |

 

## 1. Pi 0

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871624800-53.png)

## 整体理解Pi 0

### 预训练的视觉-语言模型VLM主干 + 动作专家通过「流匹配」输出动作

机器人基础模型三大挑战：大规模预训练、模型架构、训练策略

#### 大规模预训练

首先利用一个预训练的视觉-语言模型(VLM)来导入互联网规模的经验。基于VLM构建他们的模型，使其继承了语言模型和视觉-语言模型的通用知识、语义推理和问题解决能力

其次，进一步训练模型以整合机器人动作，使其成为一个视觉-语言-动作(VLA)模型。为了能够利用多种不同的机器人数据源，作者采用跨体态训练，即将多种类型机器人的数据合并到同一个模型中。

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871627625-56.png)

#### 模型架构

为了能够执行高度灵巧和复杂的物理任务，作者使用<u>**带有流匹配的动作分块架构来表示复杂的连续动作分布**</u>。

通过流匹配微调VLM以生成动作(且是多时间步的动作块)。

> 那为何要这么做呢？原因也很简单，VLM可以有效地从网络上传输语义知识，但它们经过训练只能输出**离散语言token**。灵巧的机器人操作需要π0以高频率(比如高达每秒 50 次)输出运动命令。为了提供这种级别的灵活性，**他们通过流匹配为预训练的 VLM 提供连续动作输出**

总之，这使得他们的模型能够以高达50Hz的频率控制机器人进行诸如叠衣服。且为了将流匹配与VLM结合，他们使用了一种新颖的动作专家，它通过流式输出(flow-based outputs)增强了标准VLM。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871630777-59.png)

#### 训练策略

模型首先在极大且多样化的语料库上进行预训练，然后在更狭窄且经过精心筛选的数据上进行微调，以引导出所需的行为模式。

> **流匹配（Flow Matching）**
>
> 流匹配方法能够高精度地建模复杂多峰分布，非常适合高频灵巧操作任务
>
> 训练时，随机对动作施加高斯噪声，并训练模型输出去噪向量场
>
> 推理时，从高斯噪声开始，**通过数值积分向量场生成动作序列**
>
> **不同之处在于：**
>
> 1. 流匹配直接对数据和噪声分布之间的**映射场(vector field)**进行建模，训练目标是匹配这一映射场
> 2. 扩散模型通常学习的是**每个去噪步骤的条件分布**
>



## 模型架构与模型推理

### **整体理解：PaliGemma + 动作专家 + 流匹配Flow matching**

作者首先组建了一个预训练混合数据集，该数据集由他们自有的灵巧操作数据集(涵盖7种不同的机器人配置，涉及68项任务)与Open X-Embodiment数据集(包含22种机器人的数据)的加权组合而成。

- 预训练阶段还使用了多样化的语言标签，结合了任务名称和片段标注(对子轨迹的细粒度标签，通常长度约为2秒)。预训练阶段的目的是训练一个基础模型，使其具备广泛的能力和泛化能力，但不必针对任何单一任务达到高性能。

- 对于复杂且需要灵巧操作的任务，随后采用后训练流程，利用高质量的精心策划数据将模型适配到特定的下游任务。

  他们研究了数据量较小至中等的高效后训练，以及针对如折叠衣物和移动操作等复杂任务，采用较大规模数据集的高质量后训练——即<u>微调</u>

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871634182-62.png)

π0模型主要由一个语言模型transformer骨干组成。遵循标准的**后期融合视觉语言模型（VLM）方法，图像编码器将机器人获取的图像观测嵌入到与语言token相同的嵌入空间中**。且进一步通过机器人相关的特定输入和输出——即本体感觉状态和机器人动作来进行增强。

π0使用条件流匹配来建模动作的连续分布。流匹配为他们的模型提供了高精度和多模态建模能力，使其特别适合高频率的灵巧操作任务

> 该架构的灵感来源于**Transfusion** ：该方法通过多目标训练单一transformer，其token对应的连续输出(比如机器人的动作)通过流匹配(扩散风格)损失进行监督，离散输出的token通过交叉熵损失进行监督
>
> 在Transfusion的基础上，他们还发现，为机器人特定的(动作和状态)token使用一套单独的权重能够提升性能。这种设计类似于专家混合模型（MoE）（但略有不同，本质上是是第一个expert输出KV cache，然后action expert不用单独处理图像文本，只接受kv cache然后进行特征交互，最终输出action），其中有两大模块
>
> 1. 第一大模块用于**图像和文本(比如人类指令)输入**
> 2. 第二大模块用于**机器人特定的输入(比如机器人的状态)**，和**输出(比如预测的机器人动作)**，该第二组权重称为动作专家
>
> 两大模块各司其职，各自处理各自接收到的输入
>

![image-20260201190334640](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201190334640.png)



- **动作模块Action Expert**

不是直接把自己的参数塞进VLM模型中，变成一个整体大模型来输出动作，而是**基于文本指令去噪**。action expert 根据文本指令去噪生成**具体的连续的动作**——而无需像RT-2那样对其进行离散化或token化(discretize or tokenize)

![image-20260201190654283](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201190654283.png)

![image-20260201190846249](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201190846249.png)



- **视觉-语言模型VLM骨干**

采用PaliGemma [5-PaliGemma: A versatile 3B VLM for transfer]作为他们的基础模型。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/44f146a005dc431bb1b73cf8a22adeb2.png)

1. 其中的PaliGemma是一个开源的30亿参数VLM，基于SigLIP和Gemma而构建，在模型规模和性能之间提供了便利的权衡
2. 作者为动作专家添加了3亿参数(从头初始化)。π0在实际实现时，用的gemma_300m定义的动作专家

### 深入细节：对PaliGemma的改造——增加额外的输入输出、引入流匹配时间步、加上动作专家权重

![image-20260201191247560](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201191247560.png)

- **具体的改动，分别涉及以下几点**

![image-20260201191339377](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201191339377.png)

![image-20260201191431265](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201191431265.png)

![image-20260201191452517](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201191452517.png)



### **模型推理**

![image-20260201192753400](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201192753400.png)

![image-20260201192818579](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201192818579.png)



## 数据收集以及预训练-微调方案

| 预训练                                                       | 后训练                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 目标是让模型接触到各种各样的任务，以便它能够获得广泛适用和一般的物理能力 | 目标是使模型能够熟练和流畅地执行所需的下游任务。             |
| 数据集应涵盖尽可能多的任务，并在每个任务中涵盖多样化的行为。 | 数据集则应涵盖有助于有效任务执行的行为，这些行为应表现出一致且流畅的策略。 |

 

直观地说，多样化(但质量较低)的预训练数据允许模型从错误中恢复并处理高度变化的情况，这些情况可能在高质量的后训练数据中不会出现，而后训练数据教会模型良好地执行任务

该预训练混合物由

- OXE[Open X-Embodiment]的一个子集OXE Magic Soup
- π数据集

组成。

注意，由于每个训练样本对应一个时间步——即一个元组

，——在本次讨论中，将以时间步来量化数据。

训练混合数据集中有9.1%来自开源数据集，包括

- 22个机器人数据的OXE
- Bridgev2 [BridgeData v2: A dataset for robot learning at scale]
- DROID [DROID: A large-scale in-the-wild robot manipulation dataset]

这些数据集中的机器人和任务通常配备一到两个摄像头，并采用低频率控制，频率在 2 到 10 Hz 之间。然而，这些数据集涵盖了广泛的物体和环境。

为了学习灵巧且更复杂的任务，作者还使用了来自他们自有的数据集，总计903M时间步长的数据，其中

- 106M步来自单臂机器人
- 797M步来自双臂机器人

这些数据涵盖了68个任务，每个任务都由复杂的行为组成——例如，“清理餐具”任务包括将各种各样的盘子、杯子和餐具放入餐具回收箱，以及将各种垃圾物品扔进垃圾桶

请注意，这里的任务定义与以往工作有显著不同，之前的工作通常使用任何名词和动词的组合(例如，“捡起杯子”与“捡起盘子”)视为不同的任务。因此，作者数据集中实际的行为范围远比“任务”数量所暗示的要广泛得多。

此外，由于数据集在规模上存在一定的不平衡(例如，更难的叠衣任务样本数量较多)，作者对每个任务-机器人组合按

进行加权，其中n为该组合的样本数量，从而对样本数量过多的组合进行降权

1. 配置向量
2. 和动作向量
3. 始终具有数据集中最大机器人的维度(在作者的案例中为18，以适应两个6-DoF机械臂、两个夹爪、一个移动底座和一个垂直驱动的躯干)
4. 对于配置和动作空间维度较低的机器人，对配置和动作向量进行零填充。对于少于三张图像的机器人，还会屏蔽掉缺失的图像槽
5. 在训练后阶段，使用一个较小的任务特定数据集对模型进行微调，以使其专门化用于特定的下游应用

## 2. Pi 0源码

[π0源码(openpi)剖析——从π0模型架构的实现：如何基于PaLI-Gemma和扩散策略去噪生成动作，到基于C/S架构下的模型训练与部署](https://blog.csdn.net/v_JULY_v/article/details/146068251?fromshare=blogdetail&sharetype=blogdetail&sharerId=146068251&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link)

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871683031-98.png)

 

## 3. 复现部署

### [openpi π₀ 项目部署运行逻辑（一）——综述](https://blog.csdn.net/qq_28912651/article/details/147816917?fromshare=blogdetail&sharetype=blogdetail&sharerId=147816917&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link)

#### Open Pi 综述

Open Pi 托管着由 Physical Intelligence team 发布的机器人开源模型与软件包。

1. π0本身的代码和权重
2. 特定平台上特定任务的微调checkpoint
3. 推理代码
4. 微调代码

当前仓库包含两类核心模型：

- the π₀  model - 基于流式扩散架构的视觉-语言-动作多模态模型（VLA）
- the π₀ - FAST  model - 采用 FAST 动作分词器的自回归式VLA模型

两类模型均提供了 base model checkpoints（基于10k+小时机器人数据预训练），并包含开箱即用案例及自定义数据集微调示例

#### 运行要求

 

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871686400-101.png)

#### 安装指南

克隆仓库时请同步更新子模块：

- uv ([uv](https://docs.astral.sh/uv/)) ([installation instruction](https://docs.astral.sh/uv/getting-started/installation/))
- Docker ([setup](https://github.com/Physical-Intelligence/openpi/blob/main/docs/docker.md))

 

#### 模型检查点

两个基础模型

特定平台特定任务的微调模型

#### 预训练模型推理

 

**远程推理（**[remote_inference.md](https://github.com/Physical-Intelligence/openpi/blob/main/docs/remote_inference.md)**）：**该项目提供了远程运行模型推理的 [examples and code](https://github.com/Physical-Intelligence/openpi/blob/main/docs/remote_inference.md)：模型可以在不同的服务器上运行，并通过 websocket 连接将 actions 传输给机器人。这使得在机器人之外使用更强大的 GPUs 并实现机器人端与计算资源解耦

 

#### 自定义数据微调指南

如何在自己的数据集上微调一个base model

1. 将数据格式转换为 LeRobot dataset (用于训练)
2. 配置训练参数并训练
3. 启动策略服务器并运行推理

 

此项目在以下 README 文档中提供了更多关于如何在 ALOHA 平台上进行模型微调和推理的示例：[ALOHA Simulator](https://github.com/Physical-Intelligence/openpi/blob/main/examples/aloha_sim)、[ALOHA Real](https://github.com/Physical-Intelligence/openpi/blob/main/examples/aloha_real)、[UR5](https://github.com/Physical-Intelligence/openpi/blob/main/examples/ur5)

 

### [Model 复现系列（三）π0 -- Physical Intelligence Pi-zero（Pi0）](https://blog.csdn.net/nenchoumi3119/article/details/148688800?fromshare=blogdetail&sharetype=blogdetail&sharerId=148688800&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link)

根据官方 ReadMe 文件的描述，他们使用了 Libero 数据集的格式作为输入，这个数据集是在一系列 **仿真** 环境下构建的 **单臂** 操作数据集

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871690173-104.png)

 



## 4. 微调

### [Model 复现系列（三）π0 -- Physical Intelligence Pi-zero（Pi0）](https://blog.csdn.net/nenchoumi3119/article/details/148688800?fromshare=blogdetail&sharetype=blogdetail&sharerId=148688800&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link)

#### 使用自己的数据进行微调

这部分内容是依照源码中 examples/libero/convert_libero_data_to_lerobot.py 的部分，涉及到的内容为以下两个链接：

Pi0 Convert Data to Lerobot：Fine-Tuning Base Models on Your Own Data

Pi0 Convert Data to Aloha：convert_aloha_data_to_lerobot

因为在这部分中每人使用的数据集在格式上存在不少差异，但受限于精力，我这里只对 rosbag、libero、HDF5 三种形式进行 lerobot 转译与微调；

 

【Note】：考虑到有些人是第一次使用 uv 方式管理包，在适配自己数据集的时候难免出现缺包的情况，想要不污染自己默认环境则使用下面的命令补装 uv 包，前面一定要带上 uv。[ $ uv pip install <package_name> ]

#### rosbag转译

#### 微调模型

根据 ReadMe 文件的介绍，最好在正式训练之前对数据进行一次统计以检查，这一步比较耗时目的是统计整个模型中被激活和冻结的部分，参数 --config-name 表示你想要微调配置：

【Note】：这一步首次执行是需要联网的，建议设置好环境变量后再执行。

如果你不清楚这里的配置是什么意思，可以去查看src/openpi/training/config.py 文件：

### [π0的微调——如何基于各种开源数据集、以及私有数据集微调openpi(含我司七月的微调实践及openpi在国产臂上的部署)](https://blog.csdn.net/v_JULY_v/article/details/146125555?fromshare=blogdetail&sharetype=blogdetail&sharerId=146125555&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link)



![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/65e41f2dc2774011837d41d19228f7c2.png)





# 三、Nvidia的GR00T系列

**Isaac-GR00T：** [github](https://github.com/NVIDIA/Isaac-GR00T/tree/main) 

|             |                                                              |                                                              |      |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| GR00T  N1   | [paper](https://arxiv.org/pdf/2503.14734) [精读](https://blog.csdn.net/v_JULY_v/article/details/146376514?fromshare=blogdetail&sharetype=blogdetail&sharerId=146376514&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) | 采用冻结的 Eagle VLM 搭配简化的适配器架构，并引入FLARE（未来潜变量对齐）损失函数。通过结合DreamGen 生成的合成轨迹与多源真实/仿真数据。 |      |
| GR00T  N1.5 | [website](https://research.nvidia.com/labs/gear/gr00t-n1_5/) [精读](https://blog.csdn.net/v_JULY_v/article/details/146376514?fromshare=blogdetail&sharetype=blogdetail&sharerId=146376514&sharerefer=PC&sharesource=m0_73994182&sharefrom=from_link) |                                                              |      |
| GR00T  N1.6 | [website](https://research.nvidia.com/labs/gear/gr00t-n1_6/) |                                                              |      |





## Nvidia GR00T

### **N1**

核心技术在于采用了仿生的“双系统”架构：利用预训练的视觉语言模型（Eagle-2）作为“系统2”处理低频（10Hz）的环境感知与语义推理，紧密耦合基于流匹配（Flow-matching）的Diffusion Transformer作为“系统1”来生成高频（120Hz）的闭环运动控制动作。

与 N1 类似，GR00T N1.5 也使用[NVIDIA Eagle](https://github.com/NVlabs/EAGLE) VLM 对文本和视觉观测进行编码。然后，DiT 会交叉关注来自 VLM 的视觉语言嵌入，并处理状态和带噪声的动作。

![image-20260201173819981](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201173819981.png)

为了解决机器人数据匮乏问题，它构建了“数据金字塔”训练策略：**底层利用“潜在动作（Latent Actions）”技术从海量无标签人类视频中提取通用行为先验，中层通过生成式AI合成反事实的“神经轨迹（Neural Trajectories）”和仿真数据（DexMimicGen）进行大规模扩充，顶层结合少量高质量真机遥操作数据。**

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871536723-8.png)

 

### **N1.5**

N1.5与N1类似之处在于

1. GR00T使用NVIDIA Eagle VLM对文本和视觉观察进行编码。其中N1中使用的是Eagle-2。N1.5中升级成了Eagle2.5
2. 然后，**视觉语言嵌入通过VLM生成，再由DiT进行交叉注意力处理，DiT处理状态和加噪的动作**

N1.5与N1相比的主要区别如下：

1. VLM模型在预训练和微调过程中均处于冻结状态。
2. 将视觉编码器连接到LLM的适配器MLP进行了简化，并对输入到LLM的视觉和文本标记嵌入都添加了层归一化。

![image-20260201173206790](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201173206790.png)

- FLARE损失函数（未来潜在表征对齐）

FLARE的核心是从动作去噪网络的隐藏状态中预测机器人未来观测的紧凑表示。FLARE主要包括两个关键阶段

1. 首先，预训练一个紧凑且具备动作感知能力的观测嵌入模型。虽然通用嵌入模型也可用于目标未来嵌入，但作者发现，针对下游控制任务显式优化的动作感知嵌入，因其紧凑性和任务对齐性，能带来更优的性能和效率
2. 接下来，通过引入极少量的附加token，与diffusiontransformer协同训练，这些token被优化用于预测未来观测嵌入

最后，FLARE还支持从无动作标签的数据源中学习，例如人类视频。通过利用GoPro采集的人类第一视角演示视频——使用头戴式GoPro摄像机采集的以自我视角为主的人体视频数据集。并且每个物体仅需一条真实机器人演示，FLARE成功学习了新的抓取策略，凸显了其在利用非结构化数据源进行大规模机器人学习方面的潜力

为了使DiT模块中的潜在表示能够预测未来的潜在状态，作者在输入序列中添加了M个可学习的未来token嵌入，使得该序列包含三个组成部分：

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871545667-14.png)

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871547919-17.png)

FLARE的方法类似于表示对齐（REPA）[11]在提升文本到图像扩散模型中的应用，但由于潜在世界建模的设定，存在几个重要的不同之处

1. 首先，将DiT策略与未来嵌入对齐，而不是当前观测的嵌入
2. 其次，FLARE的架构引入了可学习的未来token，使得流匹配和对齐在DiT中沿着各自独立的路径进行，并通过自注意力机制相互作用。

通过这种方式，鼓励DiT模块在保持其通过动作流匹配进行动作预测能力的同时，能够在内部推理未来的潜在状态。



###  N1.6

建筑结构变更：

- 基础视觉语言模型 (VLM)：我们使用 NVIDIA Cosmos-2B 内部的 VLM 变体。该 VLM 支持灵活的分辨率，并且可以按图像的原始宽高比进行编码，无需填充。VLM 经过训练，既可用于通用视觉语言任务，也可用于具身推理任务，例如下一步动作预测。
- 使用 2 倍大的 DiT（N1.5 中为 16 层，而 **DiT 为 32 层**）。
- 移除 N1.5 的 VLM 后 4 层 Transformer 适配器。取而代之的是，我们在预训练期间解冻 VLM 的前 4 层。
- 对于大多数实施例，**预测的是状态相对动作块，而不是绝对关节角度或 EEF 位置**。

除了 N1.5 数据混合之外，N1.6 预训练数据还包括来自以下来源的数千小时远程操作数据：

- 双手动YAM臂
- AGIBot Genie1
- 在 BEHAVIOR 套件上模拟 Galaxea R1 Pro
- 使用 Unitree G1 进行全身运动控制



## **DreamGen**

(website:https://research.nvidia.com/labs/gear/dreamgen/)

为了超越远程操作数据进行概括，使人形机器人能够在新的环境中学习新任务，我们使用[DreamGen](https://research.nvidia.com/labs/gear/dreamgen) 生成合成机器人数据进行训练。

DreamGen是一个用于生成神经轨迹的四阶段流程，该流程利用视频世界模型生成合成机器人数据。这项工作首次实现了零样本行为泛化和零样本环境泛化：我们使人形机器人能够在已知和未知环境中执行22种新行为，而仅需来自单一环境中单个抓取放置任务的远程操作数据。

通过DreamGen，我们将机器人学习的范式从扩展人类远程操作数据转变为通过世界模型扩展GPU计算能力。

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769871553964-20.png)

DreamGen分为4个步骤：

1. 我们首先在目标机器人上微调视频世界模型（图像到视频扩散模型），以学习给定机器人本体的动力学特性。
2. 我们通过**初始帧和语言指令**引导模型，生成机器人视频，这些视频不仅包含在既定领域内的行为，还包含在全新环境中的新颖行为。
3. [我们通过潜在动作模型](https://latentactionpretraining.github.io/)或[逆动力学模型](https://openai.com/index/vpt/)（IDM）提取伪机器人动作。
4. 我们将这些标有伪动作的视频（称为神经轨迹）用于**下游视觉运动策略学习**。



# 四、数据集

- **训练数据来源**

互联网数据、仿真数据、真实数据

- 如果采用**端到端模仿学习**的方法，给定一张图像并直接输出机器人动作，这通常依赖于真实世界数据
- 如果采用端到端的**强化学习RL**，因为需要一个可以反复交互的环境，则往往依赖于仿真数据

当然，也有**先在RL仿真环境里训练一个base model，然后再在真实环境中通过模仿学习微调**



| 人工收集 |          |                  |
| -------- | -------- | ---------------- |
| 手持夹爪 | 收集方便 | UMI / FastUMI    |
| 动作捕捉 | 精度较高 | DexCap           |
| 摇操作   | 精度很高 | 主从机械臂 ALOHA |
|          |          | 手持夹爪 Pika    |
|          |          | VR摇操 iDP3      |



- **数据金字塔**

![img](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/1769879768149-3.png)

**真实机器人的轨迹数据**

真实世界中通过物理机器人硬件收集的数据则是顶层

**合成生成的数据集，含视频生成数据、仿真数据**

1. 通过物理模拟生成的合成数据——即synthetic data generated with physics simulations，比如仿真数据
2. 和/或由现成的神经模型增强的数据(比如通过VLM标注的数据)，形成中间层

对于后者，他们使用预训练的视频生成模型生成合成的神经轨迹——就是模型预测的行为。通过这种方式，将内部收集的遥操作轨迹从88小时增加到827小时，即数据金字塔的“峰值”，使用了带有新语言指令的多样化反事实机器人轨迹(diverse counter factual robot trajectories)

**网络数据，与(带标注或无标注的)人类视频数据集**

大量的网络数据和人类视频构成金字塔的底层基础

当然，其中有相当一部分的图像-文本数据可以是作为VLM的预训练数据的，而当VLM赋能机器人时，VLM这部分的预训练数据便成为了机器人的知识之一

 ![image-20260201164659002](./%E6%88%91%E7%9A%84%E8%B0%83%E7%A0%94.assets/image-20260201164659002.png)



**金字塔的底层提供广泛的视觉和行为先验，而顶层则确保了在具身的真实机器人执行中的落地性**



# 五、Benchmark













# 六、边缘计算与延迟挑战

VLA模型通常拥有数十亿（7B+）参数，推理算力需求巨大。为了保证移动机器人的续航，板载计算机（如NVIDIA Jetson AGX Orin）的功耗受限。在边缘端运行量化后的7B模型，推理频率可能仅为2-5Hz，而机器人底层的运动控制需要至少100Hz-1kHz的频率。

为了解决这一矛盾，工业界普遍采用“快慢双系统”分层架构。